{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1784,
     "status": "ok",
     "timestamp": 1767716505135,
     "user": {
      "displayName": "Muhammad Shibli",
      "userId": "07847958448912945819"
     },
     "user_tz": -360
    },
    "id": "jJ7ay3Mv_Qvf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Feature Extraction Techniques\n",
    "\n",
    "This notebook covers all major feature extraction methods in NLP:\n",
    "1. **Bag of Words (CountVectorizer)**\n",
    "2. **TF-IDF Vectorizer**\n",
    "3. **N-grams (Unigram, Bigram, Trigram)**\n",
    "4. **Binary Bag of Words**\n",
    "5. **Word2Vec**\n",
    "6. **Hashing Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 118,
     "status": "ok",
     "timestamp": 1767716665332,
     "user": {
      "displayName": "Muhammad Shibli",
      "userId": "07847958448912945819"
     },
     "user_tz": -360
    },
    "id": "OTdgTveAHRyq"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':['people watch campusx', 'campusx watch campusx', 'people write comment', 'campusx write comment'], 'output':[1,1,0,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1767716670204,
     "user": {
      "displayName": "Muhammad Shibli",
      "userId": "07847958448912945819"
     },
     "user_tz": -360
    },
    "id": "mLPONCUBH2rM",
    "outputId": "9a567baf-e9c1-4a60-eaea-f40ee9402e36"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>campusx watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>campusx write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  output\n",
       "0   people watch campusx       1\n",
       "1  campusx watch campusx       1\n",
       "2   people write comment       0\n",
       "3  campusx write comment       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1767716695579,
     "user": {
      "displayName": "Muhammad Shibli",
      "userId": "07847958448912945819"
     },
     "user_tz": -360
    },
    "id": "fliYkrbnH8fo"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767716731744,
     "user": {
      "displayName": "Muhammad Shibli",
      "userId": "07847958448912945819"
     },
     "user_tz": -360
    },
    "id": "CpE27mkHH_sC"
   },
   "outputs": [],
   "source": [
    "bou = cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1767716761817,
     "user": {
      "displayName": "Muhammad Shibli",
      "userId": "07847958448912945819"
     },
     "user_tz": -360
    },
    "id": "g8owGWRoIK4D",
    "outputId": "007a5a75-3d57-45d7-9cd3-d4957ed21064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 2, 'watch': 3, 'campusx': 0, 'write': 4, 'comment': 1}\n"
     ]
    }
   ],
   "source": [
    "# print vocabulary\n",
    "\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1767716818784,
     "user": {
      "displayName": "Muhammad Shibli",
      "userId": "07847958448912945819"
     },
     "user_tz": -360
    },
    "id": "Ip_0Z4VyIS9P",
    "outputId": "d606c4fd-5a8e-4767-ef81-4e35005de475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 0]]\n",
      "[[2 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bou[0].toarray())\n",
    "print(bou[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1767716833410,
     "user": {
      "displayName": "Muhammad Shibli",
      "userId": "07847958448912945819"
     },
     "user_tz": -360
    },
    "id": "iQGfOPfZIgx1",
    "outputId": "372b7ecf-657b-46e4-925f-4062f89625ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(['campusx watch and write comment of campusx']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag of Words (CountVectorizer) - Already Demonstrated Above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF Vectorizer (Term Frequency - Inverse Document Frequency)\n",
    "TF-IDF reflects how important a word is to a document in a collection. It penalizes common words and rewards rare but important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vocabulary:\n",
      "{'people': 2, 'watch': 3, 'campusx': 0, 'write': 4, 'comment': 1}\n",
      "\n",
      "Feature Names: ['campusx' 'comment' 'people' 'watch' 'write']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = tfidf.fit_transform(df['text'])\n",
    "\n",
    "# Display vocabulary\n",
    "print(\"TF-IDF Vocabulary:\")\n",
    "print(tfidf.vocabulary_)\n",
    "print(\"\\nFeature Names:\", tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campusx</th>\n",
       "      <th>comment</th>\n",
       "      <th>people</th>\n",
       "      <th>watch</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613667</td>\n",
       "      <td>0.613667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.850816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525464</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.496816</td>\n",
       "      <td>0.613667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    campusx   comment    people     watch     write\n",
       "0  0.496816  0.000000  0.613667  0.613667  0.000000\n",
       "1  0.850816  0.000000  0.000000  0.525464  0.000000\n",
       "2  0.000000  0.577350  0.577350  0.000000  0.577350\n",
       "3  0.496816  0.613667  0.000000  0.000000  0.613667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display TF-IDF Matrix as DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\")\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for new text:\n",
      "[[0.6829022  0.42176004 0.         0.42176004 0.42176004]]\n"
     ]
    }
   ],
   "source": [
    "# Transform new text using TF-IDF\n",
    "new_text = ['campusx watch and write comment of campusx']\n",
    "tfidf_new = tfidf.transform(new_text)\n",
    "print(\"TF-IDF for new text:\")\n",
    "print(tfidf_new.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-grams (Unigram, Bigram, Trigram)\n",
    "N-grams capture word sequences. Unigram (n=1), Bigram (n=2), Trigram (n=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Features: ['campusx' 'comment' 'people' 'watch' 'write']\n",
      "Unigram Matrix:\n",
      " [[1 0 1 1 0]\n",
      " [2 0 0 1 0]\n",
      " [0 1 1 0 1]\n",
      " [1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Unigram (single words) - default\n",
    "cv_unigram = CountVectorizer(ngram_range=(1, 1))\n",
    "unigram_matrix = cv_unigram.fit_transform(df['text'])\n",
    "print(\"Unigram Features:\", cv_unigram.get_feature_names_out())\n",
    "print(\"Unigram Matrix:\\n\", unigram_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Features: ['campusx watch' 'campusx write' 'people watch' 'people write'\n",
      " 'watch campusx' 'write comment']\n",
      "\n",
      "Bigram Matrix:\n",
      " [[0 0 1 0 1 0]\n",
      " [1 0 0 0 1 0]\n",
      " [0 0 0 1 0 1]\n",
      " [0 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Bigram (two consecutive words)\n",
    "cv_bigram = CountVectorizer(ngram_range=(2, 2))\n",
    "bigram_matrix = cv_bigram.fit_transform(df['text'])\n",
    "print(\"Bigram Features:\", cv_bigram.get_feature_names_out())\n",
    "print(\"\\nBigram Matrix:\\n\", bigram_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Features: ['campusx watch campusx' 'campusx write comment' 'people watch campusx'\n",
      " 'people write comment']\n",
      "\n",
      "Trigram Matrix:\n",
      " [[0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Trigram (three consecutive words)\n",
    "cv_trigram = CountVectorizer(ngram_range=(3, 3))\n",
    "trigram_matrix = cv_trigram.fit_transform(df['text'])\n",
    "print(\"Trigram Features:\", cv_trigram.get_feature_names_out())\n",
    "print(\"\\nTrigram Matrix:\\n\", trigram_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined N-gram Features: ['campusx' 'campusx watch' 'campusx watch campusx' 'campusx write'\n",
      " 'campusx write comment' 'comment' 'people' 'people watch'\n",
      " 'people watch campusx' 'people write' 'people write comment' 'watch'\n",
      " 'watch campusx' 'write' 'write comment']\n",
      "\n",
      "Combined N-gram Matrix Shape: (4, 15)\n"
     ]
    }
   ],
   "source": [
    "# Combined N-grams (Unigram + Bigram + Trigram)\n",
    "cv_combined = CountVectorizer(ngram_range=(1, 3))\n",
    "combined_matrix = cv_combined.fit_transform(df['text'])\n",
    "print(\"Combined N-gram Features:\", cv_combined.get_feature_names_out())\n",
    "print(\"\\nCombined N-gram Matrix Shape:\", combined_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Binary Bag of Words\n",
    "Instead of word counts, it uses 1 if word is present, 0 if absent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary BoW Features: ['campusx' 'comment' 'people' 'watch' 'write']\n",
      "\n",
      "Binary Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campusx</th>\n",
       "      <th>comment</th>\n",
       "      <th>people</th>\n",
       "      <th>watch</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   campusx  comment  people  watch  write\n",
       "0        1        0       1      1      0\n",
       "1        1        0       0      1      0\n",
       "2        0        1       1      0      1\n",
       "3        1        1       0      0      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary Bag of Words\n",
    "cv_binary = CountVectorizer(binary=True)\n",
    "binary_matrix = cv_binary.fit_transform(df['text'])\n",
    "\n",
    "print(\"Binary BoW Features:\", cv_binary.get_feature_names_out())\n",
    "print(\"\\nBinary Matrix:\")\n",
    "binary_df = pd.DataFrame(binary_matrix.toarray(), columns=cv_binary.get_feature_names_out())\n",
    "binary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular BoW (count): [[2 0 0 1 0]]\n",
      "Binary BoW (0/1): [[1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Compare: Regular BoW vs Binary BoW for repeated words\n",
    "test_text = ['campusx watch campusx']  # 'campusx' appears twice\n",
    "\n",
    "# Regular Count\n",
    "regular_count = cv.transform(test_text)\n",
    "print(\"Regular BoW (count):\", regular_count.toarray())\n",
    "\n",
    "# Binary Count\n",
    "binary_count = cv_binary.transform(test_text)\n",
    "print(\"Binary BoW (0/1):\", binary_count.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hashing Vectorizer\n",
    "A memory-efficient alternative to CountVectorizer. Uses hashing to map words to features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing Vectorizer Matrix Shape: (4, 10)\n",
      "\n",
      "Hashing Matrix:\n",
      "[[0.         0.         0.         0.57735027 0.         0.\n",
      "  0.57735027 0.         0.57735027 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.89442719 0.         0.4472136  0.        ]\n",
      " [0.         0.         0.         0.89442719 0.         0.\n",
      "  0.         0.         0.4472136  0.        ]\n",
      " [0.         0.         0.         0.57735027 0.         0.\n",
      "  0.57735027 0.         0.57735027 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Initialize Hashing Vectorizer with fixed number of features\n",
    "hv = HashingVectorizer(n_features=10, alternate_sign=False)\n",
    "hash_matrix = hv.fit_transform(df['text'])\n",
    "\n",
    "print(\"Hashing Vectorizer Matrix Shape:\", hash_matrix.shape)\n",
    "print(\"\\nHashing Matrix:\")\n",
    "print(hash_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word2Vec (Word Embeddings)\n",
    "Word2Vec creates dense vector representations that capture semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: [['people', 'watch', 'campusx'], ['campusx', 'watch', 'campusx'], ['people', 'write', 'comment'], ['campusx', 'write', 'comment']]\n",
      "\n",
      "Word2Vec Vocabulary: ['campusx', 'comment', 'write', 'watch', 'people']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = [text.split() for text in df['text']]\n",
    "print(\"Tokenized Text:\", tokenized_text)\n",
    "\n",
    "# Train Word2Vec model\n",
    "# vector_size: dimension of word vectors\n",
    "# window: context window size\n",
    "# min_count: minimum word frequency\n",
    "# sg: 0 for CBOW, 1 for Skip-gram\n",
    "w2v_model = Word2Vec(sentences=tokenized_text, vector_size=5, window=2, min_count=1, sg=0)\n",
    "\n",
    "print(\"\\nWord2Vec Vocabulary:\", list(w2v_model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'campusx':\n",
      "[-0.01072454  0.00472863  0.10206699  0.18018547 -0.186059  ]\n",
      "\n",
      "Words similar to 'campusx':\n",
      "[('write', 0.4593397080898285), ('comment', 0.2018294781446457), ('watch', 0.10980252921581268)]\n"
     ]
    }
   ],
   "source": [
    "# Get vector for a specific word\n",
    "word = 'campusx'\n",
    "print(f\"Vector for '{word}':\")\n",
    "print(w2v_model.wv[word])\n",
    "\n",
    "# Find similar words\n",
    "print(f\"\\nWords similar to '{word}':\")\n",
    "print(w2v_model.wv.most_similar(word, topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Vectors Shape: (4, 5)\n",
      "\n",
      "Document Vectors:\n",
      "[[ 0.03306348  0.05455515  0.08568611  0.00991283 -0.08267251]\n",
      " [-0.01925648  0.02232962  0.07465715  0.06488888 -0.18703145]\n",
      " [ 0.05050308  0.06663775  0.07462712  0.01534326 -0.01515094]\n",
      " [-0.00181687  0.03441222  0.06359816  0.0703193  -0.11950988]]\n"
     ]
    }
   ],
   "source": [
    "# Create document vectors by averaging word vectors\n",
    "def get_document_vector(text, model):\n",
    "    words = text.split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    return np.zeros(model.vector_size)\n",
    "\n",
    "# Get document vectors for all texts\n",
    "doc_vectors = np.array([get_document_vector(text, w2v_model) for text in df['text']])\n",
    "print(\"Document Vectors Shape:\", doc_vectors.shape)\n",
    "print(\"\\nDocument Vectors:\")\n",
    "print(doc_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Word2Vec - Skip-gram Model\n",
    "Skip-gram predicts context words from the target word (opposite of CBOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram Word2Vec Vocabulary: ['campusx', 'comment', 'write', 'watch', 'people']\n",
      "\n",
      "Vector for 'campusx' (Skip-gram):\n",
      "[-0.01072454  0.00472863  0.10206699  0.18018547 -0.186059  ]\n"
     ]
    }
   ],
   "source": [
    "# Skip-gram model (sg=1)\n",
    "w2v_skipgram = Word2Vec(sentences=tokenized_text, vector_size=5, window=2, min_count=1, sg=1)\n",
    "\n",
    "print(\"Skip-gram Word2Vec Vocabulary:\", list(w2v_skipgram.wv.key_to_index.keys()))\n",
    "print(f\"\\nVector for 'campusx' (Skip-gram):\")\n",
    "print(w2v_skipgram.wv['campusx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. One-Hot Encoding\n",
    "Each word is represented as a binary vector with only one 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words: ['people', 'campusx', 'write', 'watch', 'comment']\n",
      "\n",
      "One-Hot Encoding:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campusx</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watch</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comment</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dim_0  dim_1  dim_2  dim_3  dim_4\n",
       "people     0.0    0.0    1.0    0.0    0.0\n",
       "campusx    1.0    0.0    0.0    0.0    0.0\n",
       "write      0.0    0.0    0.0    0.0    1.0\n",
       "watch      0.0    0.0    0.0    1.0    0.0\n",
       "comment    0.0    1.0    0.0    0.0    0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Encoding for words\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Get all unique words\n",
    "all_words = ' '.join(df['text']).split()\n",
    "unique_words = list(set(all_words))\n",
    "print(\"Unique Words:\", unique_words)\n",
    "\n",
    "# Label encode first\n",
    "le = LabelEncoder()\n",
    "integer_encoded = le.fit_transform(unique_words)\n",
    "\n",
    "# One-hot encode\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "integer_encoded = integer_encoded.reshape(-1, 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# Display as DataFrame\n",
    "onehot_df = pd.DataFrame(onehot_encoded, index=unique_words, columns=[f'dim_{i}' for i in range(len(unique_words))])\n",
    "print(\"\\nOne-Hot Encoding:\")\n",
    "onehot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Character-level N-grams\n",
    "Extract features at character level instead of word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character N-gram Features (first 20): [' c' ' ca' ' co' ' w' ' wa' ' wr' 'am' 'amp' 'at' 'atc' 'ca' 'cam' 'ch'\n",
      " 'ch ' 'co' 'com' 'e ' 'e c' 'e w' 'en']\n",
      "\n",
      "Total Character Features: 62\n",
      "Matrix Shape: (4, 62)\n"
     ]
    }
   ],
   "source": [
    "# Character-level N-grams (useful for spelling variations, typos)\n",
    "cv_char = CountVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "char_matrix = cv_char.fit_transform(df['text'])\n",
    "\n",
    "print(\"Character N-gram Features (first 20):\", cv_char.get_feature_names_out()[:20])\n",
    "print(\"\\nTotal Character Features:\", len(cv_char.get_feature_names_out()))\n",
    "print(\"Matrix Shape:\", char_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Additional Parameters & Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features (top 3 most frequent): ['campusx' 'comment' 'people']\n"
     ]
    }
   ],
   "source": [
    "# max_features: Limit vocabulary size to top N most frequent\n",
    "cv_max = CountVectorizer(max_features=3)\n",
    "max_matrix = cv_max.fit_transform(df['text'])\n",
    "print(\"Max Features (top 3 most frequent):\", cv_max.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With min_df=2, max_df=0.9: ['campusx' 'comment' 'people' 'watch' 'write']\n"
     ]
    }
   ],
   "source": [
    "# min_df and max_df: Control document frequency thresholds\n",
    "# min_df=2: word must appear in at least 2 documents\n",
    "# max_df=0.9: word must appear in less than 90% of documents\n",
    "cv_df = CountVectorizer(min_df=2, max_df=0.9)\n",
    "df_matrix = cv_df.fit_transform(df['text'])\n",
    "print(\"With min_df=2, max_df=0.9:\", cv_df.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without English stop words: ['campusx' 'comment' 'people' 'watch' 'write']\n",
      "Without custom stop words: ['campusx' 'comment' 'people']\n"
     ]
    }
   ],
   "source": [
    "# Stop words removal\n",
    "cv_stop = CountVectorizer(stop_words='english')\n",
    "stop_matrix = cv_stop.fit_transform(df['text'])\n",
    "print(\"Without English stop words:\", cv_stop.get_feature_names_out())\n",
    "\n",
    "# Custom stop words\n",
    "custom_stops = ['watch', 'write']\n",
    "cv_custom = CountVectorizer(stop_words=custom_stops)\n",
    "custom_matrix = cv_custom.fit_transform(df['text'])\n",
    "print(\"Without custom stop words:\", cv_custom.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Tokenizer Features: ['campusx' 'comment' 'people' 'watch' 'write']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noman/Downloads/NLP-20260106T163250Z-3-001/NLP/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Custom tokenizer with preprocessing\n",
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    # Convert to lowercase and extract words\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b[a-z]+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "cv_custom_token = CountVectorizer(tokenizer=custom_tokenizer)\n",
    "custom_token_matrix = cv_custom_token.fit_transform(df['text'])\n",
    "print(\"Custom Tokenizer Features:\", cv_custom_token.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Extraction Methods Comparison ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Type</th>\n",
       "      <th>Captures Semantics</th>\n",
       "      <th>Memory Efficient</th>\n",
       "      <th>Best Use Case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Sparse</td>\n",
       "      <td>No</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Simple text classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Sparse</td>\n",
       "      <td>No</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Document ranking, search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Binary BoW</td>\n",
       "      <td>Sparse</td>\n",
       "      <td>No</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Short texts, presence matters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N-grams</td>\n",
       "      <td>Sparse</td>\n",
       "      <td>Partial</td>\n",
       "      <td>Low</td>\n",
       "      <td>Phrase detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hashing Vectorizer</td>\n",
       "      <td>Sparse</td>\n",
       "      <td>No</td>\n",
       "      <td>High</td>\n",
       "      <td>Large-scale streaming data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Word2Vec (CBOW)</td>\n",
       "      <td>Dense</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Semantic similarity (frequent words)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Word2Vec (Skip-gram)</td>\n",
       "      <td>Dense</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Semantic similarity (rare words)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>One-Hot Encoding</td>\n",
       "      <td>Sparse</td>\n",
       "      <td>No</td>\n",
       "      <td>Low</td>\n",
       "      <td>Small vocabulary tasks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Character N-grams</td>\n",
       "      <td>Sparse</td>\n",
       "      <td>No</td>\n",
       "      <td>Low</td>\n",
       "      <td>Spelling variations, typos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Method    Type Captures Semantics Memory Efficient  \\\n",
       "0          Bag of Words  Sparse                 No           Medium   \n",
       "1                TF-IDF  Sparse                 No           Medium   \n",
       "2            Binary BoW  Sparse                 No           Medium   \n",
       "3               N-grams  Sparse            Partial              Low   \n",
       "4    Hashing Vectorizer  Sparse                 No             High   \n",
       "5       Word2Vec (CBOW)   Dense                Yes           Medium   \n",
       "6  Word2Vec (Skip-gram)   Dense                Yes           Medium   \n",
       "7      One-Hot Encoding  Sparse                 No              Low   \n",
       "8     Character N-grams  Sparse                 No              Low   \n",
       "\n",
       "                          Best Use Case  \n",
       "0            Simple text classification  \n",
       "1              Document ranking, search  \n",
       "2         Short texts, presence matters  \n",
       "3                      Phrase detection  \n",
       "4            Large-scale streaming data  \n",
       "5  Semantic similarity (frequent words)  \n",
       "6      Semantic similarity (rare words)  \n",
       "7                Small vocabulary tasks  \n",
       "8            Spelling variations, typos  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary table comparing all feature extraction methods\n",
    "summary_data = {\n",
    "    'Method': [\n",
    "        'Bag of Words', \n",
    "        'TF-IDF', \n",
    "        'Binary BoW', \n",
    "        'N-grams',\n",
    "        'Hashing Vectorizer',\n",
    "        'Word2Vec (CBOW)',\n",
    "        'Word2Vec (Skip-gram)',\n",
    "        'One-Hot Encoding',\n",
    "        'Character N-grams'\n",
    "    ],\n",
    "    'Type': ['Sparse', 'Sparse', 'Sparse', 'Sparse', 'Sparse', 'Dense', 'Dense', 'Sparse', 'Sparse'],\n",
    "    'Captures Semantics': ['No', 'No', 'No', 'Partial', 'No', 'Yes', 'Yes', 'No', 'No'],\n",
    "    'Memory Efficient': ['Medium', 'Medium', 'Medium', 'Low', 'High', 'Medium', 'Medium', 'Low', 'Low'],\n",
    "    'Best Use Case': [\n",
    "        'Simple text classification',\n",
    "        'Document ranking, search',\n",
    "        'Short texts, presence matters',\n",
    "        'Phrase detection',\n",
    "        'Large-scale streaming data',\n",
    "        'Semantic similarity (frequent words)',\n",
    "        'Semantic similarity (rare words)',\n",
    "        'Small vocabulary tasks',\n",
    "        'Spelling variations, typos'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"=== Feature Extraction Methods Comparison ===\\n\")\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNETxX0YgJdUP51JVG5dUGn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
